import os
from dotenv import load_dotenv
from pathlib import Path
from typing import Optional

env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)

class LLMConfig:
    """
    Configuration des mod√®les de langage.

    Cette classe centralise tous les param√®tres li√©s aux LLMs :
    - Cl√© API OpenAI ou Google (Gemini)
    - Mod√®le √† utiliser
    - Param√®tres de g√©n√©ration (temp√©rature, tokens max, etc.)

    PROVIDERS SUPPORT√âS:
    - openai: GPT-3.5, GPT-4
    - gemini: Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0
    """

    # Provider LLM - CHARG√â DEPUIS L'ENVIRONNEMENT
    # Options: "openai", "gemini"
    PROVIDER: str = os.getenv("LLM_PROVIDER", "gemini")

    # Cl√© API OpenAI - CHARG√âE DEPUIS L'ENVIRONNEMENT
    # JAMAIS cod√©e en dur dans le code !
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    # Cl√© API Google (Gemini) - CHARG√âE DEPUIS L'ENVIRONNEMENT
    GOOGLE_API_KEY: str = os.getenv("GOOGLE_API_KEY", "")

    # URL de base pour Ollama (si utilisation locale)
    OLLAMA_BASE_URL: str = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

    # Mod√®le par d√©faut
    # OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo
    # Gemini: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash
    DEFAULT_MODEL: str = os.getenv("LLM_MODEL", "gemini-2.0-flash")

    # Param√®tres de g√©n√©ration
    TEMPERATURE: float = 0.7  # Cr√©ativit√© (0=d√©terministe, 1=cr√©atif)
    MAX_TOKENS: int = 500     # Longueur max des r√©ponses

    @classmethod
    def validate(cls) -> bool:
        """
        V√©rifie que la configuration est valide.

        Returns:
            bool: True si la config est valide, False sinon
        """
        if cls.PROVIDER == "gemini":
            if not cls.GOOGLE_API_KEY:
                print("‚ö†Ô∏è  ATTENTION: GOOGLE_API_KEY non d√©finie !")
                print("   Veuillez cr√©er un fichier .env avec votre cl√© API.")
                print("   Obtenez-la sur: https://aistudio.google.com/apikey")
                return False
            if cls.GOOGLE_API_KEY.startswith("your"):
                print("‚ö†Ô∏è  ATTENTION: GOOGLE_API_KEY n'est pas configur√©e !")
                print("   Remplacez la valeur dans .env par votre vraie cl√©.")
                return False
            print(f"‚úÖ Provider: Gemini ({cls.DEFAULT_MODEL})")
            return True

        elif cls.PROVIDER == "openai":
            if not cls.OPENAI_API_KEY:
                print("‚ö†Ô∏è  ATTENTION: OPENAI_API_KEY non d√©finie !")
                print("   Veuillez cr√©er un fichier .env avec votre cl√© API.")
                print("   Exemple: cp .env.example .env")
                return False
            if cls.OPENAI_API_KEY.startswith("sk-your"):
                print("‚ö†Ô∏è  ATTENTION: OPENAI_API_KEY n'est pas configur√©e !")
                print("   Remplacez la valeur dans .env par votre vraie cl√©.")
                return False
            print(f"‚úÖ Provider: OpenAI ({cls.DEFAULT_MODEL})")
            return True

        else:
            print(f"‚ö†Ô∏è  Provider inconnu: {cls.PROVIDER}")
            print("   Options valides: openai, gemini")
            return False


# ================================================
# CONFIGURATION DE LA SIMULATION
# ================================================

class SimulationConfig:
    """
    Param√®tres de la simulation du th√©√¢tre de l'arnaque.
    """

    # Mode debug
    DEBUG: bool = os.getenv("DEBUG", "False").lower() == "true"

    # Fr√©quence des votes audience (tous les X tours)
    AUDIENCE_VOTE_FREQUENCY: int = 3

    # Nombre de propositions √† pr√©senter au vote
    AUDIENCE_CHOICES_COUNT: int = 3

    # Dur√©e simul√©e des pauses (en secondes textuelles)
    PAUSE_DURATION: int = 2

    # Activer/d√©sactiver les effets sonores
    SOUND_EFFECTS_ENABLED: bool = True


# ================================================
# CONFIGURATION DES SC√âNARIOS
# ================================================

class ScenarioConfig:
    """
    Configuration des sc√©narios d'arnaque disponibles.
    """

    # Sc√©narios disponibles
    AVAILABLE_SCENARIOS = [
        "tech_support",    # Arnaque support technique Microsoft
        "bank_fraud",      # Arnaque faux conseiller bancaire
        "lottery_scam",    # Arnaque √† la loterie
        "grandchild_scam", # Arnaque au petit-fils en difficult√©
    ]

    # Sc√©nario par d√©faut
    DEFAULT_SCENARIO: str = "tech_support"


# ================================================
# V√âRIFICATION AU D√âMARRAGE
# ================================================

def check_configuration() -> bool:
    """
    V√©rifie toute la configuration au d√©marrage.

    Cette fonction est appel√©e au lancement de l'application
    pour s'assurer que tout est correctement configur√©.

    Returns:
        bool: True si tout est OK, False sinon
    """
    print("üîß V√©rification de la configuration...")

    # V√©rifier la config LLM
    if not LLMConfig.validate():
        return False

    # V√©rifier que le fichier .env existe
    if not env_path.exists():
        print("‚ö†Ô∏è  Fichier .env non trouv√©.")
        print("   Cr√©ez-le avec: cp .env.example .env")
        return False

    print("‚úÖ Configuration valide !")
    return True


# ================================================
# FONCTION GET_LLM - FACTORY POUR LES MOD√àLES
# ================================================

def get_llm(model: Optional[str] = None, temperature: float = 0.7):
    """
    Factory pour cr√©er le bon LLM selon le provider configur√©.

    Args:
        model: Nom du mod√®le (optionnel, utilise DEFAULT_MODEL sinon)
        temperature: Temp√©rature de g√©n√©ration (0-1)

    Returns:
        Un objet LLM compatible LangChain (ChatOpenAI ou ChatGoogleGenerativeAI)

    Raises:
        ValueError: Si le provider n'est pas support√©
    """
    model_name = model or LLMConfig.DEFAULT_MODEL
    provider = LLMConfig.PROVIDER.lower()

    if provider == "gemini":
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=LLMConfig.GOOGLE_API_KEY,
            temperature=temperature,
            convert_system_message_to_human=True
        )

    elif provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=model_name,
            openai_api_key=LLMConfig.OPENAI_API_KEY,
            temperature=temperature,
            max_tokens=LLMConfig.MAX_TOKENS
        )

    else:
        raise ValueError(f"Provider LLM non support√©: {provider}. Utilisez 'openai' ou 'gemini'.")

llm_config = LLMConfig()
simulation_config = SimulationConfig()
scenario_config = ScenarioConfig()

